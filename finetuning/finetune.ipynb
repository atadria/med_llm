{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccacd6c-b04e-403b-b85d-c4be10debfdd",
   "metadata": {},
   "source": [
    "# Notebook for finetuning Mistral "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90a74d-d765-41f2-a12d-d8ff2a8154e1",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a143fff-ee0a-4b09-bf08-5bdb8f7860ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6de57-7101-49a4-855e-67346d19f05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80034c8e-71ce-4cf3-b895-95a2c01b7135",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad74332-008e-4580-9b48-2be1d482ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create datasets \n",
    "# from .dataset_utils import get_dataset, get_instruct_dataset\n",
    "# dataset = concatenate_datasets[\n",
    "#     get_dataset(), \n",
    "#     get_instruct_dataset()\n",
    "# ]\n",
    "\n",
    "## load datasets \n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    " \n",
    "\n",
    "dataset = concatenate_datasets([\n",
    "    load_from_disk('data/llms-405417/simple_text_med_dataset.hf'), \n",
    "    load_from_disk('data/llms-405417/instruct_med_dataset.hf')\n",
    "])\n",
    "\n",
    "dataset = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57828-2ea0-4a97-89e4-4d0671c6b3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf1b11-c11b-4c8f-aa86-1bc670078d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b41fa6-afcd-4655-8530-413c98e35eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = Tru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02f564-97d5-4324-8569-c7608c140e74",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bb39c-226c-4407-81f8-5f1bd8eabde4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = \"./med_mistral\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        gradient_checkpointing=True,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to='none', # or log to WanDB \n",
    "        logging_dir=\"./logs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094ed9b-1107-4bcd-9afd-ca9df73fe5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4c99f-4f91-4cef-85a7-711d2ee8af2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3e50f-23ab-4767-ace4-2d8a28845db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a8362-f159-474b-8a28-44b1691d7d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c8926c1-af7f-4795-a9fe-ecbd4060c040",
   "metadata": {},
   "source": [
    "## Model interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24f24-10f6-49b6-bbfd-3c124736d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[INST] {} [\\INST]\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        \"Continue the fibonnaci sequence. 1, 1, 2, 3, 5, 8 ...\"\n",
    "    ) \n",
    "    # prompt.format(\n",
    "    #     \"What are symptoms of flu?\"\n",
    "    # )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca358810-c53c-49e0-93ee-afd4daac7140",
   "metadata": {},
   "source": [
    "## Push model to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3baf3e-668f-4714-9afe-fd860eabfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"med_mistral\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model.push_to_hub_merged(\"atadria/med_mistral\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf757c96-8e1e-4f5c-a356-9f92591c8308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
